2016-07-25
==========
# Visual studio 2015 Linux extension
1. debug linux code in visual studio
2. depend on g++, gdb, gdb-server
3. Very interesting and interesting


# recent accquired
1. Samba file sharing for quickly clean training dataset
2. use Scrapy Framework to crawl training dataset
3. split terminal, Sublime
4. Python basic & Linux Basic & CentOS
5. Thinking in Style transfer
6. New topic of Computer Vision
7. Thining in Deep Learning and Resiudal Network
8. Training with Caffe and Titan X
9. C++ 11 new features
10. flask & websocket
11. inotify 


# Zotera: Reference Managment tools


# Serving Deep learning models with Nginx & Torch
1. We love Torch for it's simplicity, speed and flexibity in defining deep learning models. One of the chanllenges that we faced is that Torch uses Lua, while the rest of our api stack is written in python/go. We had to decide how to serve our results to the rest of our system, so we ended up in a simple but effective solution, which was to use nginx to create an interal rest api for model inference.
2. Today we'' be showing you how to create a simple nginx server that server torch7 models. The whole implementation is less than 100 lines of code.
3. Nginx is very high performance web server with proven performance and scalability. OpenResty is a web platform that bundles standard Nginx (http://nginx.org) with LuaJit and a lot of useful modules.
4. Requirements: Torch & OpenResty
``` Shell
./configure --with-luajit
make make install
``` 
5. To create a server with nginx you create a .conf that holds the configuration of the server, things like how many workers you want to create, how many connections per worker are avaliable, what end points to create and pretty much the whole setup of your server is defined in this file. This post is not a general tutorial about Nginx or OpenResty, a better staring option can be found here.
6. Create a folder where your server will use as a root path, in there we'll place the needed files to server our model. The folder will contain three base files:
* nginx.conf: this file holds the instructions for the nginx
* serve.lua: this file holds the lua script that will load and run model at every call
* start.sh: a script to start the server

```
error_log stderr notice;
daemon off;
events {}

http {
    lua_code_cache on;
    init_by_lua_file 'server.lua'
    server {
        listen 6788;
        lua_code_cache on;
        lua_need_request_body on;
        client_body_buffer_size 10M;
        client_max_body_size 10M;
        location /ff {
            #only perimit POST requests
            if ($request_method !~ ^(POST)$)
            return 403;
        }
        content_by_lua '
        local body = ngx.req.get_body_data()
        if body == nil then 
            ngx.say("empty body")
        else 
            local resp = FeedForward(body)
            ngx.say(cjson.encode({result=resp}))
        end 
        ';
        
        
    }
} 
```

Every request served by ngx_lua runs in a seperate Lua VM instance, so that means that every .lua file you load (via init_by_lua_file, more details in a bit) will reload at every requst. By turning lua_code_cache on files will be caches and will be load once per worker. So turning lua_code_cache on is allows us to just load our Lua model once and keep it in memeory. 
9. The next line runs the following init_by_lua_file 'server.lua' which instructs the server to run server.lua script. The server.lua script is assigned to load a model and provide a function that will allow us to pass data to the model receive responses. 

``` lua
-- load the require lua models
torch = requires("torch")
nn = require("nn")
gm = require("image")
cjson = require("cjson")

-- model 
modelPath='./model.t7'
ninputs = 3
model = nn.Sequential()
model:add(nn.Linear(ninput,2))

-- let's save a dummy model 
torch.save(modelPath, model)

-- load pretrained model
net = torch.load(modelPath)
net:evaluate()

-- our end point function
-- this function is called by the ngx server
-- accepts a json body

fucntion FeedForward(json)
print("starting")
--decode and extract field x
local data = cjson.decode(json)
local input = torch.Tenson(data.x)
local response = {}

 -- example checks 
if input == nil then print("No input given")
elseif input:size(1) ~= ninputs then print("Wrong input size")
else
    -- evaluate the input and create a response
    local output = net:forward(input):float()
    -- from tensor to table 
    for i=1, output:size(1) do 
    response[i] = output[i]
end
end
```

10. After we call the initialization of server.lua, we are ready to provide responses to the ngx server through our FeedForward function.
11. The following code will create a receiving location(localhost:6788/ff) that accepts only post requests with a json body and reply with the response of the FeedForward that was decleard within server.lua.
``` shell
/usr/local/openresty/nginx/sbin/nginx -p "$(pwd)" -c "nginx.conf" 
```


# princeple of optimization
1. 不要做优化
2. 暂时不要做优化


# FeedForward Style transfer
1. Size does matter. The result is much more appealing. Here's my picture processed in style of Kandinsky painting. Both models were trained with batchsize 1 for 2 epochs.
* First uses the default size of 256
* Second uses 512px style and square centered crops of training data, rescaled with bicubic interpolation so that the smallest side is 512. Training took 44 hours on GTX 970
2. I'm using gtx 1080 - 8GB ram and try to train with size 512 but get out of memory at feature_hat = vgg(y). The problem is cudnn. After i make cudnn correct for chainer, everthing is ok.
3. As I found substracting mean images before image transformation network is not so effective, I'd like to change codes.
4. Training by the entire database will spend too much time, Do you have experience in the impact of the number of training images? How many differences between 1w images and the entire database?
 * Is cropping meaning keep the aspect ratio invairant while resize the small side to 512, and then square centered crop it ? I replaced 256 in train.py to 512, Do I need to change the size in net.py?
 * In addition to image preprocessing, other parameters are all default settings?
Answer:
 * Reducing the dataset to 10k significantly deteriorates the quality. Definitely not recommended. 
 * Scale the images so that the smallest side is of desired value, preserving the aspect ratio, then crop. This is optional. I read the VGG models were trained like this, so I thought the same technique could be applied to styles. Changes should be done to train.py only.
 * All parameters at default. You can try increasing --lambda_tv to smth like 10e-3. Lowering it as proposed in paper tends to produce more artifacts.
7. I've done some experiments. The result is quite steady, preserving consistency between frames for moving objects / transistion, except for some jitter in the backgrounds, areas with subtle color change, compression artifacts.
8. please note, whatever the size you're training, it's highly advised to set bicubic resampling when resizing like so: resize((256, 256), 2). This will likely give a more smooth result.
9. Experience: 
    * Lowering tv tends to produce more artifacts
    * Increasing epochs to 4 smooths out the dithering pattern and reduce artifacts, but the result isn't much different apart from that. Sometimes the second epoch looks even better.

# 加水印
convert src.jpg logo.gif -gravity southeast -geometry +5+10 -composite dest.jpg


# Torch neural network
``` lua
-- Forward
local out = net:forward(images)
descriptor_net:forward(out)

-- Backward
local grad = descriptor_net:backward(out, nil)
net:backward(images, grad);

```



# TODO
1. Torch basic
2. Heisenberg style
3. Torch dl framework library
4. reimplement Perception loss with Torch

x = torch.cat(x1, x2, [dimension])
y = torch.diag(x)
torch.eye(n)
torch.histc()

x:nDimension()
x:dim() 
x:size()
y = torch.Tensor(x:size()):copy(x)

y = x:clone()


# News
1. Places365 for Scene style transfer
2. texture net update
    * To achieve the results from the paper you need to paly with -image_size, -style_size, -style_layers, -content_layers, -style_weight
    * Do not hesitate to set batch_size to one , but remember the larger batch_size the large learning rate you can use.

with open('filename') as file:
    for line in file:
        do_things(line)

for xxx in xxx  fastest    