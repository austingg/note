2016-04-07
===========
# Paper Reading (two papers)
* Deep3D
* SSD

## Deep3D: Fully Automatc 2D-to-3D Video Conversion with Deep Convolution Neural Network

1. We design a deep nerual network that takes as input the left eye's view, internally estimates a soft (probilitstic) disparity map, and then renders a novel image for the right eye. We train our model end-to-end on ground-truth stereo-frame pairs with the objective of directly predicting one view from the other. The internal disparity-like map produced by the network is computed only in service of creating a good right eye view. We conjecture that this approach is easier to trian for than the alternative of using heuistics to derive a disparity map, training the model to predict disparity directly, and then using the predicted disparity to render the new image. Our model also performs in-painting implicityly without the need for post-processing.
2. For quantitative evaluations, we use a dataset of 3D movies and report pixel-wise metrics comparing the reconstructed right view and the ground-truth right view. We also conduct human subject exeperiments to show the effctiveness of our solution.
3. Most existing automatic 2D-to-3D conversino pipelines can be roughly divided into two stages. First, a depth map is estimated from an image of the input view, then a DIBR algorithm combines the depth map with the input view to generate the missing view of a stereo pair. 
4. Early attempts to estimate depth from a single image utilize various hand engineered features and cues including defocus, scattering, and texture gradient. These methods rely on a single cue while humans perceive depth by seamlessly combining information from various sources. 
5. Most recent research has moved to learning-based methods. These approaches take single-view 2D images and their depth maps as supervision and try to learn a mapping from 2D image to depth map. Learning-based methods combine more cues and have better generalization, such as recent works that use deep convolutional neural networks to advance the state-of-the-art for this this problem. However, collecting high quality image-depth pairs is difficult, expensive, and subject to sensor-denpendent constraints.
6. The lack of volume and variations in these datasets limits the gernerality of traditional learning-based methods. Moreover, the depth maps these methods produce are only an intermediate representation and a separate DIBR step is still needed to generate the final result.
7. Monocular depth prediction is challenging and we conjecture that performing that task accurately is unnecessary. Motivated by the recent trend towards training end-to-end differentiable system. We propose a method that requires stereo pairs for training and learns to directly predict the right view from the left view.
8. Unlike 2D image/depth map pairs, there is a vast amount of training data avaliable to our approach since roughly 10-20 3D movies have been produce each year since 2008 and each has hundreds of thousands of frames.

## SSD: single shot multibox detection
1. Our approach, named SSD, discretizes the output space of bouding boxes into  a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to bettern match the object shape.
2. Our SSD model is simple relative to methods that require object proposal because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single work.
3. Current state-of-the-art object detection systems are variants of the following approach: hypothesize bouding boxes, resample pixels or features for each box, and apply a high quality classifer. Although accurate, these approaches have been too computatinally intensive for embedded systems and, even with high-end hardware, too slow for real-time or near real-time applications. 
4. This paer presents the first deep network based object detector that does not resample pixels or features for bouding box hypotheses and is as accurate as approaches that do.
5. The fundamental improvement in speed comes from eliminating bouding box proposals and the subsequent pixel or feature resampling stage. This is not he first paper to do this but by adding a series of improvements, we manage to increase the qccuracy signicantly over previous attempts.  
6. Most convolutinal networks reduce the size of feature maps at the deeper layers. Not only does this reduce computatin and memory cost but it also provides some degress of translation and scale invariance.