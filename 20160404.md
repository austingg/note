2016-04-04
==========
### thoughtly blog 
[Link](http://www.thoughtly.co/blog/)
1. Deep Learning Lession 2: Activation Function
    * Why not just call the dot product the final output of the neuron and be done? We could, and in doing so we'd actually be using the simplest activation function for the neuron, the linear activation function
    * If we use a linear activation it will be difficult to interpret the output of our neuron. For some inputs it may output a very large negative number, zero or a very large positive number. However, we typeically want to think of our neuron as detector and not a simple calculator.
    * A class of functions known as sogmoid functions meet this decription. The most famous sigmoid function, the logistic function.
    * This allows our unbounded dot product to be mapped to the significantly more useful range from zero to one.
    * Gray area
### Indico.io blog ---- Exploring Computer Vision(Part I)
[Link](https://indico.io/blog/exploring-computer-vision-convolutional-neural-nets/)
1. The following posts will discuss how we can reuse CNNs in different domains without having to train new models - a process called transfer learning.
2. Convolution is a math operation(like addition or multiplication) that combines a matrix, called the kernel, with an image to produce a new image.
3. The magic of Convolutional neural networks is that they learn how to convert images into useful features that a classifier can use for prediction. There are no hand-defined rules here. CNNs provide us with a generalized algorithm that works on a wide variety of images without requiring any domain knowledge of what it is trying to classify.
4. Transfer Learning: (another way no finetune) just use pretrained cnn as feature extractors.

### Github Related
1. [Squash your commit](https://github.com/blog/2141-squash-your-commits)
2. Github Pages: very easy and have some classical pages, should try it later. Very interesting