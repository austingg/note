2016-06-02
=========
# Ristretto : CNN approximation tool by LEPS( Laboratory for embedded processing systems)
1. Ristretto is an automated CNN-approximation tool which condenses 32-bits floating point networks. Ristretto is an extention of Caffe and allows to test, train and finetune networks with limited numerical precision.
2. Ristretto Tool: The Ristretto tool performs automatic network quantization and scoring, using different bit-widths for number representation, to find a good balance between compression rate and network accuracy.
3. Ristretto Layers: Ristretto reimplements Caffe-layers with quantized numbers.
4. Testing and Training: Thanks to Ristretto's smooth integration into Caffe, network description files can be manually changed to quantized different layers. The bit-width used for different layes as well as other parameters can be set in the network's prototxt file. This allows to directly test and train condense networks, without any need to recompilation.
5. Nets with multiple accuracy layers: The Ristretto tools condense a neural network, according to the user-defined error margin. The tool assumes that the accuracy in question is the very first accuracy score in the network description file. If you wish to trim a network according to a different accuracy score, please adjust `score_number` default value in the 


### Caffe and Linux Command
1. tee: read from standard input and write to standard output and files.
2. ./build/tools/caffe train you/solver/path 2>&1 | tee you/log/path 
3. parse prototxt can specify which line caused the problem
4. top -d: update interval
5. htop: an interactive process viewer for linux.
6. nvidia-smi -l (loop)
7. free check memory : `free -m` 
8. iostat


### Vim
1. delete multi-line: start, end`d`


### t-SNE
1. t-Distributed Stochastic Neighbor Embedding(t-SNE) is a techniques for dimensinality reduction that is particularly well suited for the visualization of high-dimensional datasets. The techniques can be implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. 
2. Developed by Laurens van der Matten and Geoffrey Hinton, it's a nonlinear dimensionality reduction techniques that is particularly well suited for embedding high-dimensional data into a space of two three dimensions, which can then be visualized in a scatter plot.
3. t-SNE is one way to tackle these high dimensional visualization problems. Instead of trying to preserve the global structure like many dimensionality reduction techniques, t-SNE tries to group local data points closer to each other, which is a better match for building human reasoning.
4. When we look at imgaes, we look to patterns and objects, not pixel intensities at pixels. It's asking too much of our dimensionality reduction algorithm to get all of that from just pixel values. Converting imgaes into some representation that preserves the content of the image is a hard problem. These features strive to be invariant under certain constrains. 
5. Now, instead of treating these models as black boxes, we start to visualize and understand them.


### Image Inpainting (图像修补)
1. Inpainting is the process of reconstructing lost or deteriorated parts of the images and videos. (**Mainly small regions or to remove small defects**)
2. OpenCV inpainting: Restores the selected region in an image using the region neighborhood. Inpainting method that could be one of the following: INPAINT_NS(Navier-Stokes based Method), INPAINT_TELEA a(ALexandru Telea 2004) (OpenCV 2.4.13 document)
3. The algorithm reconstructs the selected image area from the pixel near the area boundary. The function may be used to remove dust and scratches from a scanned photo, or to remove undesirable objects from still images or video.
4. OpenCV image inpainting tutorial: remove small noises, strokes etc in old photographs by a method called inpainting. We need to create a mask of same size as that of input image, where non-zero pixels corresponds to the area which is to be inpainted.
5. Alas: content aware filling,  Resynthesizer - Heal Selection


An image inpainting techniques based on the fast marcing method. 2004
Navies-Stokes, fluid Dynamics and image and video inpainting.  2001 


### Stacked neural network ( transfer learning:pretrained network and fine-tuning:change network parameters)
1. stacked neural networks is defined as a combination of publicly available neural network architectures whose features are extracted at an intermediate layers of the network, and then concatenated together to form a larger feature set.
2. The concatenated feature vector is used to train a classifier layer which consists of an optional dropout layer, an affine layer and an svm loss function.
3. **Joint Training of multiple tasks**. We also looked at how the generalization of network features suffers as a result of finetuning a network on given task.
4. worth thinking
5. We trained a single network with the concatenation of multi-ple datset inputs in each minibatch. Each minibatch consisted of 4 images, 2 of which were from task A and 2 from task B. The output features from the network are then fed into two independent linear classifiier layers, one for each of the tasks.

### software for split screen 
1. tmux
2. screen
3. terminator


### Rating Image Aesthetics using deep learning
1. Ascessing aesthetics of images is challening for computers because aesthetics ay be rated differently by different people and an optimal computational representation of aesthetics is not obvious. More importantly, the difficulty lies in designing a proper image representation to mpa the percepton of images to their aesthetics ratings.
2. These aesthetics-relevant features are often inspired and designed based on intuition in photography or psychology literature. However, they share some essential limitations.


### Deeply fused nets


### Python numpy
1. numpy.argmax, argsort
2. numpy.reshape, squeeze: removing single-dimensional entries form the shape of an array.
3. numpy.save, load -> *.npy
4. numpy.count_non_zero