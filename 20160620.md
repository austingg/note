2016-06-20
==========
### Linux Command
1. tee: read from standard input and write to standard output and files
  - a, -- append
2. scp: secure copy (remote file copy program), copies files between hosts on a network. It uses shh for data transfer, scp will ask for passwords if they are needed for authentication.
  - 4: forces scp to use IPV4 addresses only.
  - 6: IPv6
  - l: limits the used bandwidth, specified in Kbits/s.
  - r: recursively copy entire directories.
3. nl: add line number
4. shuf: random shuffle
5. tree
6. ls > val.lst
7. for dir in `find ./ -type d` ;
   do
      echo -n "$dir " ;
      find $dir -type f | wc -l;
      echo ""
   done
   find ./ -type d | wc -l
   find ./ -type f | wc -l
   ls | wc -l
8. ls -l | sort -u | head -10
  
### Greate Software
1. sudo apt-get install vim-scripts
2. sudo apt-get install ctags
3. sudo apt-get install vim-addons-manager
4. vim-addons install omnicppcomplete
5. vim-addons install taglist
6. vim-addons install vinmanager
7. sudo apt-get install cscope : like ctags but more powerful

### Caffe
1. iter: its value should always indicate the number of times the weights have been updated.


### Nivida News
1. the new GPU Inference Engine: powerful tools that make it even easier to create solutions on your platform.
2. Nvidia DIGITS 4 introduces a new object detection workflow, enabling data scientists to train deep nerual networks to find faces, pedestrains, traffic signs, vehicles and other objects in a sea of images.
3. GIE can combine layers both vertically and horizontally into a single optimized kernel, the Caffe timing shown for each bar is the sum of Caffe kernels corresponding to each fued GIE kernel.
4. During the build phase GIE indentifies opportunities to optimize the network, and in the deployment phase GIE urns the optimized network in a way that minimizes latency and maximizeds throughput.
5. GIE is currently being evaluated under an Early Access (EA) Program.
6. Using GIE, cloud service providers can more efficiently process images, video and other data in their hyperscale data center production environments with high thouput.
7. Inference: the next step in GPU-Accelerated Deep Learning. inference typically batches a smaller number of inputs than training, as services relying on inference to work. latency becomes important for inference as well.
8. GPUs also benefit from an improvement contributed to the Caffe framework to allow it to use cuBlas instead of GEMM for inference when the batch size is 1.

### ICML residual network tutorial
1. Deep Residual Network: A simple and clean framework of training "very" deep nets.
2. From shallow to deep
3. From 10 layers to 1000 layers
  - 5 layers: easy
  - >10 layers: initialization, Batch Normalization
  - >30 layers: skip connections
  - >100 layers: identity skip connections
4. Both forward(response) and backward (gradient) signal can vanish/explode
  - 22-layer ReLU net: good init converges faster
  - 30-layer ReLU net: good init is able to converge
5. BN: 
  - normalizing each layer, for each mini-batch
  - greatly accelerate training
  - Less sensitive to initialization
  - improve regularization
6. A practival design of going deeper: bottleneck, similar complexity
7. Representation, Optimization, Generalization
8. Pre-activation ResNet, keep the shortest path as smooth as possible
  - Keep the shortest path as smooth as possible
  - Features of any layers are additive outcomes
  - 1000-layer ResNets can be easily trained and have better accuracy
9. Features matter
  - Our results are all based on ResNet-101
  - Deeper features are well transferrable
#### Reference
1. Efficient Backprop
2. Understanding the difficulty of training deep feedforward neural networks.


### unify encoding between Machines and XShell
1. locale
2. /etc/sysconfi/i18n then reconnect 