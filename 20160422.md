2016-04-22
========
# Training Deepp Nets with Sublinear Memory Cost 
0. [Code](https://github.com/dmlc/mxnet-memonger)
1. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory giving a more memory efficient training algorithm with a little extra computation cost. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30% additional running time cost on ImageNet problems.
2. We mainly focus on reducing the memory cost to store intermediate resuls (feature maps) and gradients, as the size of the parameters are relatively small comparing to the size of the intermediate feature maps in many common deep architectures.
3. Reducing memory consumption not only allows us to train bigger models. It also enables larger batch size and better device utilization. For memory limited devices, it helps improve memory locality and potentially leads to better memory access patterns.
4. We can trace the idea of computational graph and liveness analysis back to the literatures of compiler optimizations. Analogy between optimizing a computer program and optimizing a deep neural network computational graph can be found. For example, memory allocation in deep networks is similar to register allocation in a compiler. The formal analysis of computational graph allows us save memory in a principled way.
5. Theano is a pioneering framework to bring the computation graph to deep learning, which is joined by recently introduced frameworks such as CNTK, Tensorflow, and MXNet. Theano and Tensorflow use reference count based recycling and runtime garbage collection to manage memory during training, while MXNet uses a static memory allocation strategy prior to the actual computation. 
6. We propose an automatic planning algorithm to provide a good memory plan for real use-cases.
7. When training a deep convolutional/recurrent network, a great propottion of the memory is usually used to store the intermediate outputs and gradients. Each of these intermedia results corresponds to a node in the graph. A smart allocation algorithm is able to assign the least amount of memory to these nodes by sharing memory when possible. Two types on memory optimizations can be used
    * Inplace operation: Directory store the output values to memory of a input value.
    * Memory sharing: Memory used by intermediate results that are no longer needed can be recycled and used in another node.
8. We can only share memory between the nodes whose lifetime do not overlap. There are multiple ways to solve this problem. One option is to construct the conflicting graph of  with each variable as node and edges between variables with overlapping lifespan and then run a graph-coloring algorithm. This will cost O(n^2) computation time. We adopt a simpler heuristic with only O(n) time. It traverses the graph in toplogical order, and uses a counter to indicate the liveness of each record. An inplace operation can happen when there is no other pending operations that depend on its input. Memory sharing happens when a recycled tag is used by another node. This can also serve as a dynamic runtime algorithm that traverse the graph, and use a garage collector to recycle the outdated memory. We use this as a static memroy allocation algorithm, to allocate the memory to each node before the execution starts, in order to avoid the overhead of garbage collection during runtime.
### Trade Computation for Memory
9. The techniques introduced in Sec. 3 can reduce the memroy footprint for both trainning and prediction of deep neural networks. However, due to the fact that most gradient operators will depend on the intermediate results of the forward pass, we still need O(n) memory for intermediate results to train a n layer convolutional network or a recurrent neural networks with a sequence of length n. In order to further recue the memory, we propose to drop some of the intermediate results, and recover them from an extra forward computation when needed.
10. More specifically during the backpropagation phase, we can re-compute the dropped intermediate results by running forward from the closest recorded results.  The algorithm only remembers the output of each segment and drops all the intermediate results within each segment. The dropped results are recomputed at the segment level during back-propagation. As a result, we only need to pay the memory cost to store the outputs of each segment plus the maximum memory cost to do backpropagation on each segment.
11. One quick application of general methodology is to drop the results of low cost operations and keep the results that are time consuming to compute. This is usually useful in a Conv-BatchNorm-Activation pipeline in convolutional neural networks. We can always keep the result of convolution, but drop the result of the batch normailization, activation function and pooling. In practice this will translate to a memory saving with little computation overhead, as the computation for both batch normalization and activatin function are cheap.
12. Setting k = sqrt(n), we get the cost of O(2sqrt(n). This algorithm only requires an additional forward pass during training, but reduces the memory cost to be sub-linear. Since the backward operation is nearly twices as time consuming as the forward one, it only slows down the computation by a small amount.
13. In the most general case, the memory cost of each layer is not the same, so we cannot simply set k = sqrt(n). However, the trade-off between the intermediate outputs and the cost of each stage still holds.
14. Optimization type:
    * no optimization
    * inplace
    * sharing
    * drop bn-relu
    * sublinear plan
 15. Because of the double forward cost in gradient calculation, the sublinear allocation strategy costs 30% runtime compared to the normal strategy. By paying in small price, we are not able to train a much wider range of deep learning models.

# MxNet ---- Training Deep Net on 14 Million Images by Using A Single Machine
1. The raw full ImageNet dataset is more than 1TB. Before training the network, we need to shuffle these images then load batch images to feed the neural network. Before we describe how we solve it, let's do some calculation first:

| Device                    | 4K Random Seek        | Sequential Seek |
| ------------------------- | --------------------- | --------------- |
| WD Black (HDD)            | 0.43 MB /s (110 IOPS) | 170 MB/s        |
| Samsung 850 PRO (SSD)     | 40 MB/s (10,000 IOPS) | 550 MB/s        |

A very naive approach is loading from a list by random seeking. If use this approach, we will spend 667 hours with HDD or 6.7 hours with SSD respectively. This is only about read. Although SSD looks not bad, but 1TB SSD is not affordable for everyone.

But we notice sequential seek is much faster than random seek. Also, loading batch by batch is a sequential action. Can we make a change ? The answer is we can't do sequential seek directly. We need random shuffle the training data first, then pack them into a sequential binary package.

This is the normal solution used by moset deep learning packages. However, unlike ImageNet 1K dataset, where we cannot store the images in raw pixels format. Because otherwise we will need more than 1TB space. Instead, we need to pack the images in compressed format.

### The Key Ingredients are
* Store the images in jpeg format, and pack them into binary record.
* Split the list, and pack several record files, instead of on file .
    - This allows us to pack the images in distributed fashion, because we will be eventually bounded by the IO cost during packing.
    - We need to make the package being able to read from several record files, which is not too hard. This will allow us to store the entire imagenet dataset in around 250G space.
 After packing, together with threaded buffer iterator, we can simply achieve an IO speed of around 3,000 images/sec on a normal HDD.
 
 # Run MXNet on Multiple CPU/GPUs 
 [mxnet document] (https://mxnet.readthedocs.org/en/latest/how_to/multi_devices.html)
 ## Data Parallelism 
 1. In default MXNet uses data parallelism to partition to the workload over multiple devices. Assume there are n devices, then each one will get the complete model and train it on 1/n of the data. The results such as the gradient and updated model are communicated cross these devices.
 2. If using data parallelism, MXNet will evenly partition a minibatch in each GPUs. Assume we train with batch size b and k GPUs, then in one iteration each GPU will perform forward and backward on a batch with size b/k. The gradients are then summed over all GPUs before updating the model. 
 3. In ideal case, k GPUs will provide k time speed up comparing to the single GPU. In addition, assume the model has size m and the temporal workspace is t, then the memory footprint of each GPU will be m + t/k. In other words, we can use a large batch size for multiple GPUs.
 4. model = mx.model.FeedForward(ctx=[mx.gpu(0), mx.gpu(2)], ...). 
 5. Advanced Usage : If the GPU are have different computation power, we can partition the workload according to their powers. For example, if GPU 0, is 3 times faster than GPU 2, then we provide an additional workload option work_load_list=[3, 1].
 
 # STRIVING For simplicity: the all convolutional net
 1. Most modern convolutional nerual networks used for object recognition are built using the same principles: Alternating convolution max-pooling layers followed by a small number of fully connected layers.
 2. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline.
 3. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks.
 4. We conceive: a homogeneous network solely consisting of convolutional layers, with occasional dimensionality reduction by using a stride of 2.  Surprisingly, we find that this basic architecture - trained using vanilla stochastic gradient descent with momentum - reaches state of the art performance without the need for complicated activation functions, any response normalization or max-pooling.