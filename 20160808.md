2016-08-08
==========
# Handbook of Face Recoginition
1. A face recognition system generally consists of four moudles: detection, alignment, feature extraction, and matching, where localizatin and normalization(face detection and alignment) are processing steps before face recognition is performed.
2. Face detection segments the face ares from the background. In the case of video, the detected faces may need to be tracked using a face tracking component. Face alignment is aimed at achieving more accurate localization and at normalizing faces thereby whereas face detection provides coarse estimates of the location and scale of each deteced face. 
3. Facial components, such as eyes, nose, and mouth and facial outline, are located; based on the location points, the input face image is normalized with respect to geometrical properties, such as size and pose, using geometrical transforms or morphing. The face is ususally further normalized with respect to photometrical properties such illumination and gray scale.
4. A face image is efficiently represented as a feature vector of low dimensionality. The features in such subspace provide more salient and richer information for recognition than the raw image. The use of subspace modeling techniques has significantly advanced face recognition technology.
5. Face detection can be considered as a task of distinguishing between the face and nonface manifolds in the image (subwindow) space and face recognition between those of individuals in the face mainfold.
6. Face detection is the first step in automated face recognition. Its reliability has a major influence on the performance and usabiliity of the entire face recognition system.
7. With appearance-based methods, face detection is treated as a problem of classifying each scanned subwindow as one of two classes(i.e., face and nonface). Appearance-based methods avoid difficiulties in modeling 3D structures of faces by considering possible face appearances under various conditions. Building such a classifier is possible because pixels on a face are highly correlated, whereas those in a nonface subwindow present much less regularity.
8. AdaBoost is used to solve the following three fundamental problems: 
    * learning effective features from a large feature set
    * constructing weak classifiers, each of which is based on one of the selected features
    * boosting the weak classifiers to construct a strong classifier. 
   Weak classifiers are based on simple scalar  Haar wavelet-like features, which are steerable filter. Viola and Jones make use of several techniques for effective computation of a large number of such features under varying scale and location , which is important for real time performance.
   Moreover, the simple-to-complex cascade of classifiers makes the computation even more efficient, which follows the principles of pattern rejection, and coarse to find search.
9. A new boosting algorithm, called FloatBoost, is proposed to incorporate Floating Search into AdaBoost(RealBoost). The backtrack mechanism in the algorithm allows deletions of weak classifiers that are ineffective in terms of the error rate, leadinig a strong classifier consisting of only a small number of weak classifiers. 
10. Normalization of pixel intensity helps correct variations in imaging parameters in cameras as well as changes in illumination conditions. The meaning of resizing is apparent; intensity normalization operations, including mean value normalization, histogram equalization, and illumination correction.
11. In the case of discrete AdaBoost, the simplest type of weak classifiers is a "stump". A stump is a single-node decision tree. When the feature is real-valued, a stump may be constructed by thresholding the value of the selected feature at a certain threshold value; when the feature is discrete-valued, it may be obtained according to the discrete label of the feature. A more general decision tree (with more than one node) composed of several stumps leads to a more sophisticated weak classifier.
12.  The stump is determined by comparing the selected feature zk with a threshold Tk as follows.


# TODO
1. tzutalin / lableImg, play with it
2. warpaffine
3. sublime text 
4. codeblocks
5. pycharm
6. python ipython notebook 
7. fb-caffe-exts, fix batchnorm layers
8. my texture nets
9. pynetbuilder
10. faster rcnn proposal layer c++ reimplement


# News
1. MKL DNN library


# plot ROC curve and getting the AUC is as simple as:
``` python 
import matplotlib.pyplot as plt
import numpy as np
x = # false_positive_rate
y = # true_positive_rate

# This is the ROC curve
plt.plot(x, y)
plt.show()

# This is the ROC curve
AUC = np.trapz(y, x)
``` 
ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the "ideal" point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a large are under the curve (AUC) is usually better. The steepness of ROC curves is also important, since it is ideal to maximize the true positive rate while minimizing the false positive rate.

ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC are to multi-calss or multi-label classification, it is necessary to binarize the output. One ROC curve can be drawn per label, but one can also draw a ROC curve by considiering each element of the label indicator matrix as a binary prediction.

We generate TP(t), TN(t), FP(t), FN(t) using a discrete number of thresholds. From these we generate FPR(t), TPR(t) and the ROC curve.

``` python
thresholds = np.linspace(1, 0, 101)
ROC = np.zeros((101, 2))

for i in range(101)
    t = thresholds[i]

    TP_t = np.logcial_and(T > t, Y == 1).sum()
    TN_t = np.logical_and(T <= t, Y == 0).sum()
    FP_t = np.logical_and(T > t, Y == 0).sum()
    FN_t = np.logical_and(T <= t, Y == 1).sum()

    FPR_t = FP_t / float(FP_t + TN_t)
    ROC[i, 0] = FRP_t
    TPR_t = TP_t / float(TP_t + FN_t)
    ROC[i, 1] = TPR_t

fig = plt.figure(figsize=(6,6))
plt.plot(ROC[:, 0], ROC[:, 1], lw=2)
plt.xlim(-0.1, 1.1)
plt.ylim(-0.1, 1.1)
plt.xlabel('$FPR(t)$')
plt.ylabel('$TPR(t)$')
plt.grid()

```


# python numpy
1. np.trapz()
2. np.r_
3. np.where
4. numpy.squeeze: Remove single-dimensional entries from the shape of an array.

parser.add_argument(type=int)
pprint.pprint(cfg)
caffe.set_mode_gpu()
caffe.set_device(args.gpu_id)

os.path.exists
os.path.basename
os.path.dirname
os.pathabspath

cPickle
mutipleprocessing


faster RCNN rbg
# Iterations for each training stage
max_iters = [80000, 40000, 80000, 40000]

``` python
def _init_caffe(cfg):
    import caffe
    np.random.seed(cfg.RNG_SEED)
    caffe.set_random_seed(cfg.RNG_SEED)

    caffe.set_mode_gpu()
    caffe.set_device(cfg.GPU_ID)
    
```

pycaffe doesn't reliable free GPU memory when instantiated nets are discarded. To work around this issue, each training stage is executed in a separate process using multiprocessing.Process.

from sklearn import svm
from utils.timer import Timer

``` python
def recall_at(t):
    ind = np.where(thresholds > t - 1e-5)[0][0]
    assert np.isclose(thresholds[ind], t)
    return recalls[ind]
print 'Recall@0.5: {:.3f}'.format(recall_at(0.5))
print 'Recall@0.6: {:.3f}'.format(recall_at(0.6))
print 'Recall@0.7: {:.3f}'.format(recall_at(0.7))
print 'Recall@0.8: {:.3f}'.format(recall_at(0.8))
print 'Recall@0.9: {:.3f}'.format(recall_at(0.9))

# print again for easy spreadsheet copying
print '{:.3f}'.format(ar)
print '{:.3f}'.format(recall_at(0.5))
print '{:.3f}'.format(recall_at(0.6))
print '{:.3f}'.format(recall_at(0.7))
print '{:.3f}'.format(recall_at(0.8))
print '{:.3f}'.format(recall_at(0.9))
```


python setup.py install --user
pkg-config --libs opencv
pkg-config --cflags opencv

回家确认一下种子问题这周末,数据提前准备好。

emplace_back vs. push_bac

