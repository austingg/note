2016-06-16
==========
### Caffe proto
1. solver:
  - train_net_param : inline train net params, just one protofile
  - debug_info
  - test_initialization 
  - snapshot_after_train 
2. Convolution:
  - dilation: factor used to dilate the kernel.
3. Data:
  - Prefetch queue (Number of batches to prefetch to host memory, default 4)
4. PoolingParam
  - PoolMethod: MAX, AVE, STOCHASTIC
  - global_pooling: true


### News about caffe and multi-gpu
1. TCC model for Windows
2. LMDB Caching will increase the memory usage. Something need to more precise.


### RHEL and CentOS
1. mkdir -p 
2. free
3. iostat
4. top/htop
5. pidstat
6. df/du
7. nohup: nohup command > myout.file 2>&1 &

8. passwd
9. useradd: you should never be logging on to a server as root.
10. chown
11. usermod -a -G sudo deploy
12. rm ./--resize
``` shell
useradd deploy
mkdir /home/deploy
mkdir /home/deploy/.ssh
chmod 700 /home/deploy/.ssh

```


### TODO (mostly this Saturday)
1. configure Samba to share files between Windows and Ubuntu
2. configure nvidia digits, my own digital caffe/mxnet
3. investigate ssd for training task.
4. Reorginized favoriate collections.
5. Softmax vs SoftmaxWithLoss
6. StrawryPi
7. MI phone clear for develop machine
8. fb-caffe-ext, batch-normalization

