2016-06-14
==========
# In deep learning, architecture engineering is the new feature engineering
1. While deep learning has simplified feature engineering in many cases, it certainly hasn't removed it. As feature engineering has decreased, the architectures of the machine learning models themselves have become increasingly more complex. Most of the time, these model architetures are as specific to given task as feature engineering used to be.
2. Architeture engineering is more general than feature engineering and provides many new opportunities.

### Fine-tuning a Pretrained Network for Style Recognition
1. The adavantage of this approcach is that, since pre-trained networks are learned on a large set of images, the intermediate layers capture the "semantics" of the general visual appearnce. Think of it as a very powerful generic visual feature that can treat as a back box. On top of that, only a relatively small amount of data is needed for good performance on the target task.
``` python
list(np.loadtxt(style_label_file, str, delimiter='\n')
print 'style label: \n', ', '.join(style_labels)


def disp_preds(net, image, labels, k=5, name='ImageNet'):
    input_blob = net.blobs['data']
    net.blobs['data'].data[0, ...] = image
    probs = net.forward(start='conv1')['probs'][0]
    top_k = (-probs).argsort()[:k]
    print 'top %d predicted %s labels =' % (k, name)
    print '\n'.join('\t(%d) %5.2f%% %s' %(i+1, 100*probs[p], labes[o]) for i, p in emerate(top_k)


# load the mean Imagenet image
mu = np.load('ilsvrc_2012_mean.npy')
mu = mu.mean(1).mean(1)  # average over pixels to obtain the mean (BGR) pixel values
print 'mean-subtracted values:', zip('BGR', mu)

transformer = caffe.io.transformer({'data':net.blobs['data'].data.shape})
transformer.set_transpose('data', (2, 0, 1))
transformer.set_mean('data', mu)
transformer.set_raw_scale('data', 255)


for layer_name, blob in net.blobs.iteritem():
  print layer_name + '\t' + str(blob.data.shape)

for layer_name, param in net.params.iteritem():
  print layer_name + '\t' + str(param[0].data.shape), str(param[1].data.shape)
  

image = caffe.io.load_image('imgae.jpg')
net.blobs['data'].data[...] = transformer.preprocess('data', image)

#perform classification
net.forward()

#obtrain the output probabilities
output_prob = net.blobs['prob'].data[0]

# sort top five predictions from softmax output
top_inds = output_prob.argsort()[::-1][:5]

zip(output_prob[top_inds], labels[top_inds])
```

### python Tempfile
1. If your application needs a temporary file to store data, but does not need to share that file with other programs, the best of option for creating the file is the TemporaryFile() function. It creates a file , and on platforms where it is possible, unlinks it immediately. This makes it impossible for another program to find or open the file, since there is no reference to it in the filesystem table. the file created by TemporaryFile() is removed automatically when it is closed.


### ML-advice
1. Good machine learning pratics: Error analysis. try to understand waht your sources of error are.
2. Getting started on a problem
  - Apporach #1: Careful design
     * Spend a long term designing exactly the right features, collecting the right dataset and designing the right algorithmic architecture.
     * Implement it and hope it works.
  - Approach #2: Build and fix
     * Implement something quick and dirty
     * Run error analyses and diagnostics to see what's wrong with it, and fix its errors.
     
     
### With humans in the loop
1. We treat the labeling process for each category as a binary classification problem - i.e., every image downloaded for the category is labeled as either positive or negative.
2. We train a classifier on the training set and then run it on both the labeled testing set and the full unlabeled pool. By examining the scores output by the classifier on the labeled testing set, we can determin score ranges which delineate either confident scores corresponding to easy images or weaker scores corresponding to more difficult or ambiguous images.
3. To define these score ranges, we compute two thresholds: the score above which 95% of the images are ground truth positives and the score below which there are only 1% of the total positive images. Both in the labeled subset and the unlabeled pool, images scoring above or below this threshold are are labeled as positive negative, respectively. All images scoring in between these two thresholds, i.e., the more challenging images, are sent to the next iteration. Thus, during each iteration, we label a portion of images with confident scores as positive and also remove highly probable negative images while mainting a good recall for the final labeled set.
4. Labeling Instructions:
5. As we wish to build a natural image dataset, we advise the workers to mark as negative any computer-generated or cartoon imagery.
6. If an object of interest is printed on a magazine/book/TV screen within an image, we consider this to be negative. In these cases it is actually the magazine/book/TV screen that is physically present within the scene, not the object of interest.


### Decision tree
1. The term Classification And Regression Tree (CART) analysis term used to refer to both of the above procedures, first introduced by Breiman. Trees used for regression and trees used for classification have some simlarities - but also some differences, such as the procedure used to determine where to split.
2. Some techniques, often called ensemble methods, construct more than one decision tree:
  - Bagging decision trees, an early ensemble method, builds multiple decision trees by repeatedly resampling training data with replacement and voting the trees for a consenus prediction.
  - Random Forest classifiers uses a numbers of decision trees
  - Boosted Trees.
3. A decision tree is a binary tree (tree where each non-leaf node has two child nodes). It can be used either for calssification or for regression. For classification, each tree leaf is marked with a class label; multiple leaves may have the same label. For regression, a constant is also assigned to each tree leaf, so the approximation function is piecewise constant.
4. Random trees is a collection (ensemble) of tree predictors that is called forest further in this section. The classification works as follows: the random trees classifier takes the input features vector, classifies it with every tree in the forest, and outputs the class label that received the majority of "votes". In case of a regression, the calssifier response is the average of the response over laa the trees in the forest. all the trees are trained with the same parameters but on different training sets. These sets are generated from the original trainng set using the bootstrap procedure: for each training set, you randomly select the same number of vectos as in the original set. 


### Linux Command
1. find: search for files in a drectory hierachy
  - depth : 
  - daystart 
  - amin: last accessed n minutes ago
  - cmin: changed
  - mmin: modified
  - group name
  - user uname
  - type : block, character, directory p named pip, file reular file
  
  ACTIONS: 
  - delete
  - exec command ; echo {} \;  {}: placeholder ; : end
  
2. sed: stream editor for filerting and transforming text
  sed "s/my/Hao Chen's/g" pets.txt > hao_pets.txt
  sed -i "s/my/Hao Chen's/g" pets.txt
  sed 's/^/$/g' pets.txt
  sed 's/$/ --- /g' pets.txt