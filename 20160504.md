2016-05-04
========
# CrossEntropy is a reasonable choice for multi mutual exclusive calssification


# TensorFlow's quantized eight bit support ---- first version release
1. I was pushing hard to git it in before the Embeded Vsion Summit, because it's especially important for low-power and mobile devices, so it's exciting to get it out there.
2. When modern neural networks were being developed, the biggest challenge was getting them to work at all! That meant that accuracy and speed during training were the top priorities. Using floating point arithmetic was the easiest way to preserve accuracy, and GPUs were well-equipped to acclerate those calculations, so it's natural that not much attention was paid to other numerical formats.
3. These days, we actually have a lot of models being developyed in commerical applications. The computation demands of training grow with the number of researchers, but the cycles needed for inference expand in proportion to users. That means pure inferece efficiency has become a burning issue for a lot of teams.
4. That is where quantization comes in. It's an umbrella term that covers a lot of different techniques to store numbers and perform calculations on them in more compact formats than 32-bit floating point.

### Why does Quantization Work ?

### Why Quantize ?
1. Neural nwtork models can take up a lot of space on disk, with the original AlexNet being over 200 MB in float format for example. Almost all of that size is taken up with the weights for neural connections, since there are often many millions of these in a single model. Because they're all sightly different floating point numbers, simple compression formats like zip don't compress them well. 
2. The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 with the range. For example with the -3.0 to 6.0 range, a 0 beyte would represent -3.0, a 255 would stand for 6.0. This means you can get the benefit of a file on disk that's shrunk by 75% and then convert back to float after loading so that your existing floating-point code can work without any changes.
3. Another reason to quanztize is to reduce the computational resources you need to do the inference calculations, by runing them entirely with eight-bit inputs and outputs. 