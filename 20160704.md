2016-07-04
==========
### TODO
1. Hacker's delight(2002)
2. linuxperftools
3. Scrapy: python crawler. 
    * Pillow -> image thumbnail
    * ImagesPipeline: spider -> pipeline -> scheduler and downloader'
    * convert all downloaded image to JPG format and RGB mode
    * avoid duplicately download image
    * generate thumbnail
    * filter image by width and height
     ``` python
        import scrapy
        class MyItem(scrapy.Item):
            image_urls = scrapy.Field()
            images = scrapy.Field()


        import scrapy
        from scrapy.contrib.pipeline.images import ImagesPipeline
        from scrapy.exceptions import DropItem

        class MyImagesPipeline(ImagesPipeline):

            def get_media_requests(self, item, info):
                for image_url in item['image_urls']:
                    yield scrapy.Request(image_url)

            def item_completed(self, results, item, info):
                image_paths = [x['path'] for ok, x in results if ok]
                if not image_paths:
                    raise DropItem("Item contains no images")
                item['image_paths'] = image_paths
            return item
    Settings.py
        ITEM_PIPELINES = {'scrapy.contrib.pipeline.images.ImagesPiplline' : 1}
        IMAGES_STORE = 'path/to/valid/dir'

        # filter out small images
        IMAGES_MIN_HEIGHT = 110
        IMAGES_MIN_WIDTH = 110

     ```
     * create Scrapy project
     * define Items to extract
     * write spider and extract items
     * write Item Pipeline to save item

     scrapy startproject tutorial
     
     scrapy.cfg
     tutorial/items.py: item definition
     tutorial/pipelines.py: pipelines definition
     tutorial/settings.py: setting file

### Terminal executation
from scrapy import cmdline
cmdline.execute("scrapy crawl dbbook". split())       


### Scrapy basic concept
1. Command line tools
    * Scrapy -> drive command
    * startproject : create scrapy project
    * genspider [-t template] <name> <domain>
    * crawl : run specified spider
    * list : list all spider
2. Items: structual data for crawling. Item is simple containers, like map.
3. Spider: define how to crawl specified net. define craw action and analysis web page
    * Spider can accept argument to extend its functions, get the argument in constructor
    * scrapy crawl myspider -a category=electionics
    * member name: the key to locate spider
    * allowed_domains
    * start_urls
    * Common spider builtin: CrawlSpider(scrapy.contrib.spiders.CrawlSpider)
    * XML, CSV, Sitemap
4. Selector: parse web page. In Scrapy, we use XPath or CSS to parse HTML file
5. Item Pipeline. The Item crawled by Spider, is sent to Item Pipeline, and will do some post processing.
    * check item data
    * remove duplicate
    * save
    * ITEM_PIPELINES = {'xxx.xxx.xxxPipeline': n}

```python
#Custom Spider
def __init__(self, category=None, *args, **kwargs):
    super(MySpider, self).__init__(*args, **kwargs)
    self.start_urls = ['http://www.example.com/categories/%s' % category]

class JsonWriterPipeline(object):
    def __init__(sefl):
        self.file = open('item.jl', 'wb')

    def process_item(self, item, spider):
        line = json.dumps(dict(item)) + "\n"
        self.file.write(line)
        return item

import pymongo

class MongoPipeline(object):
    def __int__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'), 
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
        
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        collections_name = item.__class__.__name__
        self.db[collection_name].insert(dict(item))
        return item


from scrapy.exceptions import DropItem
class DuplicatesPipeline(object):

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        if item['id'] in self.ids_seen:
            raise DropItem("Duplicate item found : %s" % item)
        else:
            self.ids_seen.add(item['id'])
            return item


```