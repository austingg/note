2016-04-27
========
# Makeup like a superstar: Deep Localized Makeup Transfer Network
1. Makeup makes the people more attractive, and there are more and more commerical facial makeup systems in the market. However, all these softwares rely on the pre-determined cosmetics which cannot meet up with users' individual needs.
2. The similarity is measured by the Euclidean distance between deep features produced by an off-the-shelf deep face recognition network.
3. Both brefore-makeup and reference faces are fed into a face parsing network to generate two corresponding labelmaps. The pasing network is based on the Fully convolutional networks by emphasizing the makeup relevant facial parts, such as eye shadow and considering the symmetry structure of the frontal faces.
4. The after-makeup face is initialized as the before-makeup face, then gradually updated via Stochastic Gradient Descent to produce naturally looking results.
5. The VGG-Face model is trained based on VGG-Very-Deep-16 CNN architecture and aims to accurately identify different people regardless of whether she puts on makeup, which meets our requirement.
6. Then the eys shadows are warped by the thin plate spline.
7. The overall energy function is optimized via Stochastic Gradient Descent (SGD) by using momentum. The proposed model can transfer the makeup in 6 seconds for an 224x224 image pair using GTX TITAN X GPU.

### Related Paper
1. Digital face makeup by example CVPR. 2009
2. Principal warp: Thin-plate splines the decomposition of deformations. PAMI. 1989
3. Wow! you are so beautyful today! ACMM 2014.
4. Deep face recognition. BMVC.2015
5. Example-based cosmetic transfer. CGA.2007


# Trade of tricks ---- Stochastic Gradient Descent Tricks
1. Use stochastic gradient descent when training time is the bottoleneck
2. Randomly shuffle the training examples. Although the theory calls for pick examples randomly, it is ususlly faster to zip sequentiall through the training set. But this does not work if the examples are grouped by class or come in a particular order. Randomly shuffling the examples eliminates this source of problems.
3. Monitor both the training cost and the validation error.
4. Check the gradients using finite difference.
5. Experiment with the learning rates using small sample of the training set.