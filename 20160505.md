2016-05-05
========
### Multilabel classification on PASCAL using python data-layers
1. Multilabel classification is a generalization of multiclass classification, where each instance (image) can belong to many classes. For example, an image may both belong to a "beach" category and a "vacation pictures" category. In multiclass classification, on the ohter hand, each image belongs to a single class.
2. Caffe supports multilabel classification through the SigmoidCrossEntropyLoss layer, and we will load data using a Python data layer. Data could also be provided through HDF5 or LMDB data layers, but the python data lyaer provides endless flexibility, so that's what we will use.
3. Define network prototxts.
``` python
def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1):
    conv = L.Convolution(bottom, kernel_size=ks, stride=stride, num_output=nout, pad=pad, group=group)
    return conv, L.ReLU(conv, in_place=True)
    
def fc_relu(bottom, nout):
    fc = L.InnerProduct(bottom, num_output=nout)
    return fc, L.ReLU(fc, in_place=True)

def max_pool(bottom, ks, stride=1):
    return L.Pooling(bottom, pool=P.Pooling.Max, kernel_size=ks, stride=stride)
    
def caffenet_multilabel(data_layer_params, datalayer):
    n = caffe.NetSpec()
    n.data, n.label = L.Python(module = 'pascal_multilabel_datalayers', layer = datalayers, ntop=2, param_str=str(data_layer_params))
    
    n.conv1,  n.relu1 = conv_relu(n.data, 11, 96, stride=4)
    n.pool1 = max_pool(n.relu1, 3, stride=2)
    n.norm1 = L.LRN(n.pool1. local_size = 5, alpha = 1e-4, beta = 0.75)
    
    
    
    
    n.score = L.InnerProduct(n.drop7, num_output=20)
    n.loss = L.SigmoidCrossEntropyLoss(n.score, n.label)
    
    return str(n.to_proto())
```


### Caffe new upate ---- FCN for segmentation
1. official version
2. fcn.berkeleyvision.org github, PAMI version is coming



### python logger
logger = loggin.getLogger()
logger.setLevel(logging.DEBUG)
sh = logging.StreamHandler()
sh.setLevel(logging.DEBUG)
formatter = logging.Formater('%(asctime)s - %(levelname)s = %(message)s')
sh.setFormatter(formatter)
logger.addHandler(sh)


### [...] usage 

### cudnn v5
1. BatchNormalization: BN in cuDNN is confiremed to be working properly in v5. So we are switching the default BN engine to cuDNN. In compliance with cuDNN's API, the BN layer will store std again instead of storing inverse std.
2. Convolution: cuDNN v5 brings WINOGRAD algorithm, wich is ultra fast for 3x3 sized convolutions. It will be chosen automatically in our planning system when possible.


### How caffe choose cudnn implementation when USE_CUDNN is specified
如果CUDNN宏有定义， 默认引擎就是CUDNN， 通过覆盖实现 
``` C++
template <typename Dtype>
shared_ptr<Layer<Dtype> > GetConvolutionLayer(const LayerParameter& param) {
        ConvolutionParameter_Engine engine = param.convolution_param().engine();
        if (engine == ConvolutionParam_Engine_DEFAULT) {
               engine == ConvolutionParam_Engine_CAFFE;
        #ifdef USE_CUDNN
            engine = ConvolutionalParameter_Engine_CUDNN;
        #endif
        }
        
        if (engine == ConvolutionParam_Engine_CAFFE) {
            return shared_ptr<Layer<Dtype> > (new ConvolutionLayer<Dtype> (param));
        #ifdef USE_CUDNN
        } else if  (engine == ConvolutionParameter_Engine_CUDNN) {
            return shared_ptr<Layer<Dtype> > (new CuDNNConvolutionLayer<Dtype> param));
        #endif
        } else {
            LOG(FATAL) << " Layer " << param.name() << " has unknown engine. ";
        }
       
       
       REGISTER_LAYER_CREATOR(Convolution, GetConvolutionLayer);
       
       
       
       
       
       
#define REGISTER_LAYER_CREATOR(type, creator)  \
    static LayerRegisterer<float> g_creator_f_##type(#type, creator<float>) ;\
    static LayerRegisterer<double> g_creator_d_##type(#type, creator<double>);\
    
```