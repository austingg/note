Paper Reading
=============
# DeepLab (PAMI2015)
1. 


# Deep Residual Network


# Perceptual losses for real time style transfer and super resolution
1. One approach for solving image transformation tasks is to train a feedforward convolutional neural network in a supervised manner, using a pixel-pixel loss function to measure the difference between output and ground-truth images.
2. However, the per-pixel losses used by these methods do not capture perceptual differences between output and ground-truth images.
3. Images are generated by minimizing a loss function.
4. In this paper we combine the benefits of these two approaches. We train feed-forward transformation networks for image transformatin tasks, but rather than usinig per-pixel loss functin depending only on low-level pixel information, we train our networks using perceptual loss functions that depend on high-level features from a pretrained loss network.
5. perceptual losses measure image similarities more robustly than per-pixel losses, and at test-time the transfromation networks run in real-time.
6. We do not use any pooling layers, instead using strided and fractionally strided convolutions for in-network downsampling and upsampling.
7. Our network body consists of five residual blocks using the architecture. All non-residual convolutional layers are followed by spatial batch normalization and ReLU nonlinearities with the exception of the output layer, which instead uses a scaled tanh to ensure that output image has pixel of in the range [0, 255].
8. Residual connections are 
9. Our style transfer networks are trained on the Microsoft COCO dataset. We resize each of the 80K training images to 256x256 and train our networks with a batch of 4 for 4w iterations, giving roughly two epochs over the training data. We use Adam with a learning rate of 1e-3.  The output images are regularized with total variation regularization with a strength of between 1e-6 and 1e-4, chosen via cross-validation per style target. 
10. We do not used weight decay or dropout, as the model does not overfit within two epochs.
11. Training takes roughly 4 hours on a single GTX Titan X GPU.
12. Although our models are trained with 256x256 images, they can be applied in a fully convolutional manner to images of any size at test-time.
13. Our style transfer networks are trained to preserve VGG-16 features, and in doing so they learn to preserve people and animals more than background objects.
14. Our method achieves a loss comparable to between 50 and 100 iterations of explicit optimization.