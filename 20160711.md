2016-07-11
==========
# Image Retrieval
1. Deep Image Retrieval Learning global representations for image search
2. A practival guide to CNNs and Fisher Vectors for image instance retrieval
3. Deep learning of binary hash codes for fast image retrieval
4. Deep hashing for faster image retrieval. CVPRw. 2015.
5. Binary code Ranking with weighted Hamming Distance
6. Supervised Learning of Semantics Preserving hashing via deep neural networks for large scale image search
7. Deep Hashing for compact binary codes learning. CVPR. 2015.

unzip -O cp936  encoding
delete file by inum 


# Visual Tracking  
1. KCF: High-Speed Tracking with Kernelized Correlation Filters. TPAMI.2015. ECCV.2012
2. CCT: BMCV.2015
3. Accurate Scale Estimation for Robust Visual Tracking. BMVC, 2014.
4. Tracking-Learning-Detection (TLD)  TPAMI. 2012
5. online object tracking: A benchmark. CVPR 2012 Visual Tracker Benchmark.

## Comparative trackers:
* KCF: PAMI2015
* TGPR: ECCV2014
* DSST: BMVC2014
* CN: CVPR2014
* Struck: ICCV2011, 
* SCM: CVPR2012
* Efficient Online Structured Output Learning for keypoint-based object tracking. CVPR. 2012
* VTD: CVPR2010
* TLD: CVPR2010
* opencv contrib tracking


# Linux Command
1. ls -i -t -rt (sort by time or decent order)
2. sort
3. cp `python do.sh` out3 (Very interesting)

# python numpy
1. shape, arange, linspace, logspace
2. zeros, ones(), empty()
3. sum, mean, average, min, max, argmax, argmin,sort
4. unique, bincount, histogram

``` python
sample_list = [line + '\n' for line in sample_list]
outfile.wirtelines(sample_list)
```
### linux file name contains space
1. tr " " "_"
2. rename 's/[ ]\+/_/g' *
3. diff dir1 dir2
4. Shell String operation

|expression|description|
|:--------:|:---------:|
|${var} | 取值|
|${#string}| length of string|
|${string:4}| string from the 4|
|${string regex}| operate string with regex|



# TODO
1. Faster CNN, roi layer c++ implementation
2. use pycaffe to generate prototxt
3. DeepLab PAMI paper
4. Winograd nervana blog
5. Algorithm Part II cousera 
6. Object tracking CSDN blog
7. Python Standard Library

# FaceCraft
1. We keep a very low positive sample ratio during stage one. Because this can decrease  false positives, which also accelerates the following negative mining stages.


# Deep residual learning for image recognition
1. Deeper neural networks are more diffcult to train. We present a residual learning framework to ease the training of the networks that are substantially deeper than those used previously.
2. Deep networks naturally integrate low/mid/high-level features and classifiers in an end-to-end multi-layer fashion, and the "levels" of features can be enriched by the number of tacked layers(depth).
3. Detection: replace VGG-16 with ResNet101
4. good init converges faster (22-layer, ReLU net), good init is able to converge (30 layers ReLU Net)
5. BN: 
    * Greatly accelerate training
    * Less sensitive to initialization
    * improve regularization
    
    
# Nervana Winograd blog II
1. FFT works best for large filters, though, and not the commonly used small 3x3 filter, which is where the Winograd algorithm comes in.
2. These techniques, combined with careful assembler level optimization for an efficient implementation make it possible to obtrain a speedup of close to 4x over direct convolution on individual layers. In practice, a factor of 2 speedup for training an entire model is easily attainbale. 

# Vacabulary
1. ascend descend

# Image Quality Access
1. Convolutional Neural Networks for No-Reference Image Quality Assessment

# CUDA and BatchSize
1. 32 threads as an unit|