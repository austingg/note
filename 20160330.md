2016-03-30
===============
### Instance FCN
* We develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifer of a relative position to instances.
1. Image-centric strategy




### Revisiting Batch Normalization for Practical Domain Adaptation

* It is still a common (yet inconvenient) practice to prepare at least tens of thousands of labled image to finetune a network on every task before the model is ready to use.
* Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning.
* Training a DNN for a new image recognition task is expensive. It requires a large amount of labeled training images that are not easy to obtain. 
* On common practise is to use a training set from a different source. For instance, one can borrow training data from an existing dataset, or query images from the search engines and then label them using Amazon Turk.

* Known as domain adaptation, the effort to bridge the gap between training and testing data dristribution has been discussed several times under the context of deep learning. 
* We hypothesize that the lable related knowledge is stored in the weight matrix of each layer, whereas domain knowledge is reprented by the statistics of the Batch Normalization layer.
* To transfer the learned representations to a new dataset, pre-training and fine-tuning have become de facto procedures. However, adapation by fine-tuning is far from perfect. It requires a consiferable amount of labeled data from the target domain, and non-negligible computational resources to re-train the whole network.

* The BN layer is originally designed to alleviate the issue of internal covariance shifting - a common problem while training a very deep neural network. If first standardizes each feature in a mini-batch, and then learn a common slop and a bias for each mini-batch.
* if training data are shuffled at each epoch, one training sample is transformed, or augmented differently in each epoch. This prpoperty acts as an additional regularization to combat against overfitting. In the testing phase, we use the global statistics instead of the statistics from one mini-batch and stabilize the testing results.
* Since BN layer can both reduce internal covariant shift and overfitting, it significantly reduces the number of iteration to converge, and improve the final performance at the same time. BN layer has become a standard component in recent top-performing CNN architectures, such as deep residual network and Inception V3.
* BN's core idea is about aligning training data from different distributions. From this perspective, it is interesting to examine the BN parameters(batch-wise mean and variance) over different dataset at different layers of the network.

* Pilot experiment suggests:
 	- Both shallow layers and deep layers exhibit domain shift in DNN, thus only adapting the last layer is not enough. Deep adaptation is a must.
 	- The statistics of BN layer is a good indication of domains.

### Script to simplify trained net by incorporating every BN to Conv and BN to FC
[utils](https://github.com/szagoruyko/imagine-nn/blob/utils/utils.lua)

```Lua
find BN and previous conv
local conv = net:get(i-1)
local bn = v
net:remove(i)

local no = conv.nOutPlane
local conv_w = conv.weight:view(no, -1)

	if bn.running_var then
		bn.running_std = bn.running_var:add(bn.eps):pow(-0.5)
	end
	if not conv.bias then
		conv.bias = bn.running_mean:clone():zero()
		conv.gradBias = conv.bias:clone()
	end

	conv_w:cmul(bn.running_std:view(no, -1):expandAs(conv_w))
	conv.bias:add(-1, bn.running_mean):cmul(bn.running_std)

	if bn.affine then
		conv.bias:cmul(bn.weight):add(bn.bias)
		conv_w:cmul(bn.weight:view(no, -1):expandAs(conv_w))
```