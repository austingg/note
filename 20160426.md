2016-04-25
========
# Dissecting the Gradient Descent method
1. Gradient Descent is a mathematical optimization method used to minimize functins in the form of a sum.
2. This method lies at the core of the mathematical tools used in Machine Learning. For instance, in a Linear Regression task, in the Ordinary Least Squares implementation, the function to be minimized (error function) is the one describing the sum of the squared residuals between the observed value and the value predicted by the fitting line, over all observations:
3. The gradient of the function is indead used to identify the direction of  maximum growth of the function.
4. The stochastic version of the Gradient Descent method does not use all points at each iteration to calculate the gradient of the function but rather pickes one point, randomly extracted from the dataset, to compute said gradient. The Storchasitc Gradient Descent will be particularly useful for large datasets where the standard updating rule might be too slow to compute.


# MXNet modules
* Runtime Dependency Engine: Schedules and executes the operations according to their read/write dependency.
* Storage Allocator: Efficiently allocate and recycles memory blocks for GPU and CPU
* Resource Manager: Manage global resources such as random number generator, temporal space
* NDArray: Dynamic asynchronize n-dimensional arrays, provide flexible imperative programs for MXNet.
* Symbolic Exectution: Static symbolic graph executor, provide efficient symbolic graph execution and optimization.
* Operator: Operators that defines static forward and gradient calculation (backprop).
* Symbol Construction: Symbolic construction, provide a way to construct computation graph (net configuration)
* KVStore: Keyp-value store interface for easy parameter synchronizations.
* Data Loading (IO): Efficient distributed data loading and augmentation.

### How to Read the Code
* All the module interface are listed in include, these interfaces are heavily documented.
* You read the Doxygen Version of the document.
* Each module will only depend on other module by the header files in include.
* The implementation of module is in src folder.
* Each source code only sees the file within its folder, src/common and include.

Most modules are mostly self-contained, with interface dependency on engine. So you are free to pick the on you are interested in, and read that part.

# Squeeze the Memory Consumption of Deep Learning
One important theme about deep learning is to train deeper and larger nets. While the hardware has been upgraded rapidly in recent years, the huge deepnet monsters are always hungry about the GPU RAMS. Being able to use less memory for the same net also means we can use larger batch size, and usually higher GPU utilization rate.

The major difference in these library comes to how to do they calculate gradient. There are mainly two ways, doing back-propagation on the same graph, or have an explicit backward path that calculates the gradients needed.

Libraries like caffe, cxxnet, torch uses the backprop on same graph. While libraries like Theano, CGT takes the explicit backward path approach. We will adopt the explicit backward path way in the article, beacuse it brings several advantages in turns of optimization.

# Amalgamation: Make the Whole System into a Single File
The idea of amalgation comes from SQLite and other projects, which packs all the codes into a single source file. Then it is only needed to compile that single file to create the library, which makes porting to various platforms much easier. MXNet provides an amalgamation script, thanks to Jack Deng, to combiles all codes needed for prediction using trained DL models into a single .cc file, which has around 30K lines of codes. The only dependency required is just a BLAS library.

# pixel-art 
[nucl.ai](https://nucl.ai/blog/enhance-pixel-art/)
1. Neural Doodle is in face a very simple project with 550 lines of code, powered by deep neural network libraries like Lasagne and Theano. However, the algorithm can be used in variety of different ways: texture synthesis, style transfer, image analogy, and now another: example-based upscaling.
2. Depending on how you use the script, it starts with different types of seed images: random noise, the larget image, or hand-crafted seed. In the case of Minescraft, the pixelated art is taken as the seed at the target resolution(e.g. 512x512) and the optimization adjusts each pixel towards the target style - neural patch by neural patch.

# Linux
1. LD_LIBRARY_PATH
2. echo $PATH